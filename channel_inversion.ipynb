{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3405f783-4bc0-426d-95ec-251124f2368f",
   "metadata": {},
   "source": [
    "## Target: find a local quantum channel that inverses dthe local ephasing channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f33139e-c195-4b80-b840-ee0a8b86c5ad",
   "metadata": {},
   "source": [
    "First we load in the MPS calculation results from ITensors calculation since this is the only package that I know would calculate PBC DMRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ef62e8-c53a-4a43-9f59-178c7a8c9b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Jul. 1\n",
    "#@YZhang\n",
    "import numpy as np\n",
    "import math\n",
    "#from scipy import linalg as la\n",
    "from numpy import linalg as la\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "import h5py\n",
    "import quimb.tensor as qtn\n",
    "import quimb as qu\n",
    "import torch\n",
    "from vqe_functions import*\n",
    "#Implement a vqe circuit\n",
    "\n",
    "\n",
    "from torch import optim\n",
    "import tqdm\n",
    "import cotengra as ctg\n",
    "\n",
    "from quimb.tensor.decomp import _ISOMETRIZE_METHODS\n",
    "from autoray import (\n",
    "    backend_like,\n",
    "    compose,\n",
    "    do,\n",
    "    reshape,\n",
    ")\n",
    "opti = ctg.ReusableHyperOptimizer(\n",
    "    progbar=True,\n",
    "    methods=['greedy'],\n",
    "    reconf_opts={},\n",
    "    max_repeats=32, \n",
    "    optlib='random',\n",
    "    # directory=  # set this for persistent cache\n",
    ")\n",
    "#opti = None\n",
    "@compose\n",
    "def isometrize_qr_fixed(x, backend=None):\n",
    "    \"\"\"Perform isometrization using the QR decomposition. FIX FOR NEW PYTORCH\"\"\"\n",
    "    with backend_like(backend):\n",
    "        Q, R = do(\"linalg.qr\", x)\n",
    "        # stabilize qr by fixing diagonal of R in canonical, positive form (we\n",
    "        # don't actaully do anything to R, just absorb the necessary sign -> Q)\n",
    "        rd = do(\"diag\", R)\n",
    "        s = do(\"sgn\", rd) + (rd == 0)\n",
    "        Q = Q * reshape(s, (1, -1))\n",
    "        return Q\n",
    "_ISOMETRIZE_METHODS[\"qr\"] = isometrize_qr_fixed\n",
    "\n",
    "# Function to load MPS tensors from HDF5\n",
    "def load_mps_from_hdf5(filename):\n",
    "    mps_tensors = []\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        for i in range(len(f.keys())):\n",
    "            mps_tensors.append(np.array(f[f'tensor_{i+1}']))\n",
    "    return mps_tensors\n",
    "\n",
    "def construct_mps(mps_tensors):\n",
    "    tn = qtn.TensorNetwork()\n",
    "    for i in range(0,len(mps_tensors)):\n",
    "        # wtf is the indexing in ITensors? so weird. I have to make several cases\n",
    "        if i ==0:\n",
    "            inds = ('p0', 'b0',)# p for 'physical' and b for 'bond' \n",
    "        elif i ==len(mps_tensors)-1:\n",
    "            inds = (f'b{i-1}',f'p{i}')\n",
    "        elif i ==1:\n",
    "            inds = ('b0','p1', 'b1')\n",
    "        else:\n",
    "            inds = (f'b{i-1}',f'b{i}',f'p{i}')\n",
    "        tensor = qtn.Tensor(mps_tensors[i], inds,tags='MPS')\n",
    "        tn = tn&tensor\n",
    "    return tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b1a1118-6667-4c51-a12a-89cbd4b141e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and check nomalization; grateful that it worked out!\n",
    "L = 8\n",
    "mps_tensors = load_mps_from_hdf5(f\"results/mps_data{L}.h5\")\n",
    "gs = construct_mps(mps_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166052a-a78d-4dc2-b8c1-15e51858d1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347e6a2b-20a9-480c-8d75-e7739c1b3760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.179555247932146"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0 = np.array([[ 1.,  0.,],[ 0,  1]])\n",
    "p1 = np.array([[ 0.,  1.,],[ 1,  0]])\n",
    "p2 = np.array([[ 0, -1j],[1j, 0]],dtype = 'complex128')\n",
    "p3 = np.array([[ 1.,  0.,],[ 0, -1]])\n",
    "paulis = [p0,p1,p2,p3]\n",
    "\n",
    "#tn =qtn.MPS_rand_state(L=L, bond_dim=2, dtype = 'complex128')\n",
    "def apply_channel(mps,pauli =p0,p_channel =0):\n",
    "    #let's now make some noise!\n",
    "    mps_c = mps.H\n",
    "    mps_c = mps_c.reindex({f'p{i}':f'pc{i}' for i in range(len(mps.tensors))})\n",
    "    rho= mps&mps_c#)^all\n",
    "    \n",
    "    p_rel_tensor = np.zeros([2,2,2])+0j\n",
    "    p_rel_tensor[0] = np.eye(2)\n",
    "    p_rel_tensor[1] = pauli\n",
    "#\n",
    "    pc_rel_tensor = np.zeros([2,2,2])+0j\n",
    "    pc_rel_tensor[0] = (1-p_channel)*np.eye(2)\n",
    "    pc_rel_tensor[1] = p_channel*pauli\n",
    "    #noise = p_channel*np.kron(pauli,pauli).reshape(2,2,2,2)\n",
    "    \n",
    "    #delta = np.einsum('ij,jl,jn->iln', np.eye(2,2), np.eye(2,2),np.eye(2,2))\n",
    "\n",
    "    for i in range (len(mps.tensors)):\n",
    "        #ind = f'k{i}',f'kc{i}',f'kc_ch{i}',f'k_ch{i}',\n",
    "        #t = qtn.Tensor(noise,ind,tags=f'site{i}')\n",
    "        inds_d = f'a{i}',f'p{i}',f'p_ch{i}',#ch for 'channelled'\n",
    "        t = qtn.Tensor(p_rel_tensor,inds_d,tags='N')\n",
    "        inds_dc = f'a{i}',f'pc_ch{i}',f'pc{i}',\n",
    "        tc = qtn.Tensor(pc_rel_tensor,inds_dc,tags='N')\n",
    "        #d1_ind =f'a{i}',f'ac{i}',f'l{i}',\n",
    "        #d1 = qtn.Tensor(delta,d1_ind,tags='del')\n",
    "        #prob = qtn.Tensor([1-p_channel,p_channel], inds = (f'l{i}',))\n",
    "        rho = rho&t&tc#&d1&prob\n",
    "\n",
    "    return rho\n",
    "\n",
    "rho = apply_channel(gs,pauli =p2,p_channel =.11)\n",
    "rho@rho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a548f7-2049-4b86-9d61-f40aff0f143e",
   "metadata": {},
   "source": [
    "### now we need to define a channel-inversion circuit. we use a local quantum circuit for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08bdd3f-ee5d-40a1-994e-00e928f87e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda47520-a512-4360-aaf6-ddd1aa626d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the order: physical, ancilla, ....\n",
    "def overlap(pqc: qtn.TensorNetwork,rho,gs):\n",
    "    for i in range (num_qubits):\n",
    "        #if i<num_qubits//2:\n",
    "        #    pqc = pqc.reindex({f'k{i}':f'p_ch{i}'})\n",
    "        #    pqc = pqc.reindex({psi_pqc.tensors[i].inds[-1]:f'p_gs{i}'})\n",
    "        #else:\n",
    "        #    pqc = pqc.reindex({f'k{i}':f'a_in{i-num_qubits//2}'})\n",
    "        #    pqc = pqc.reindex({psi_pqc.tensors[i].inds[-1]:f'a_out{i-num_qubits//2}'})\n",
    "        if i%2:\n",
    "            pqc = pqc.reindex({f'k{i}':f'p_ch{i//2}'})\n",
    "            pqc = pqc.reindex({psi_pqc.tensors[i].inds[-1]:f'p_gs{i//2}'})\n",
    "        else:\n",
    "            pqc = pqc.reindex({f'k{i}':f'a_in{i//2}'})\n",
    "            pqc = pqc.reindex({psi_pqc.tensors[i].inds[-1]:f'a_out{i//2}'})\n",
    "    \n",
    "    pqc_c = pqc.H\n",
    "    for i in range (num_qubits//2):\n",
    "        pqc_c = pqc_c.reindex({f'a_in{i}':f'ac_in{i}'})\n",
    "        pqc_c = pqc_c.reindex({f'p_gs{i}':f'pc_gs{i}'})\n",
    "        pqc_c = pqc_c.reindex({f'p_ch{i}':f'pc_ch{i}'})\n",
    "        \n",
    "    for i in range(L):\n",
    "        prod = qtn.Tensor(np.array([1,0]),inds = (f'a_in{i}',),tags = 'A')\n",
    "        prod_c = qtn.Tensor(np.array([1,0]),inds = (f'ac_in{i}',),tags = 'A')\n",
    "        prod.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.complex128))\n",
    "        prod_c.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.complex128))\n",
    "        rho = rho&prod&prod_c\n",
    "    gs_c = gs.H\n",
    "    gs = gs.reindex({f'p{i}':f'p_gs{i}' for i in range(L)})\n",
    "    gs_c = gs_c.reindex({f'p{i}':f'pc_gs{i}' for i in range(L)})\n",
    "    #(gs_c&gs&rho&pqc_c&pqc).draw(['U','N','MPS','A'])\n",
    "    return -1*abs((gs_c&gs&rho&pqc_c&pqc).contract(optimize=opti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df1dc23-ad9c-4bfd-be91-adbdd1be8e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, pqc,rho,gs):\n",
    "        super().__init__()\n",
    "\n",
    "        # extract the raw arrays and a skeleton of the TN\n",
    "        params, self.skeleton = qtn.pack(pqc)\n",
    "        # n.b. you might want to do extra processing here to e.g. store each\n",
    "        # parameter as a reshaped matrix (from left_inds -> right_inds), for\n",
    "        # some optimizers, and for some torch parametrizations\n",
    "        self.torch_params = torch.nn.ParameterDict({\n",
    "            # torch requires strings as keys\n",
    "            str(i): torch.nn.Parameter(initial)\n",
    "            for i, initial in params.items()\n",
    "        })\n",
    "        self._loss_fn = lambda x: overlap(x,rho,gs)\n",
    "\n",
    "    def forward(self):\n",
    "        # convert back to original int key format\n",
    "        params = {int(i): p for i, p in self.torch_params.items()}\n",
    "        # reconstruct the TN with the new parameters\n",
    "        pqc = qtn.unpack(params, self.skeleton)\n",
    "        # isometrize and then return the energy\n",
    "        return self._loss_fn(pqc.isometrize(method='qr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "238bb636-3701-4855-8d48-f3f1fa96e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 2*L\n",
    "depth = 2\n",
    "\n",
    "rho.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.complex128))\n",
    "gs.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.complex128))\n",
    "\n",
    "psi_pqc = qmps_f(num_qubits, in_depth= depth, n_Qbit=num_qubits-1, qmps_structure=\"brickwall\", canon=\"left\",val_iden = 0.0,rand = False)\n",
    "pqc = psi_pqc.tensors[num_qubits]\n",
    "for i in range (num_qubits+1,len(psi_pqc.tensors)):\n",
    "    pqc = pqc&psi_pqc.tensors[i] #extrating the pqc part\n",
    "pqc.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.complex128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ebb10d0-ec87-43c5-8a48-2ba2e988e52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                     | 0/32 [00:00<?, ?it/s]/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "F=7.11 C=8.15 S=17.00 P=18.01: 100%|█████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.3963, dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap(pqc,rho,gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c1feb56-1080-42a7-a196-f0657db4718f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                    | 0/500 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                                     | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "F=7.26 C=8.26 S=19.00 P=19.59:   3%|██▍                                                                           | 1/32 [00:00<00:18,  1.67it/s]\u001b[A\n",
      "F=7.26 C=8.26 S=19.00 P=19.59:   6%|████▉                                                                         | 2/32 [00:00<00:12,  2.40it/s]\u001b[A\n",
      "F=7.22 C=8.09 S=18.00 P=19.00:  16%|████████████▏                                                                 | 5/32 [00:01<00:03,  6.86it/s]\u001b[A\n",
      "F=7.22 C=8.09 S=18.00 P=19.00:  22%|█████████████████                                                             | 7/32 [00:01<00:03,  8.25it/s]\u001b[A\n",
      "F=7.22 C=8.09 S=18.00 P=19.00:  28%|█████████████████████▉                                                        | 9/32 [00:01<00:03,  6.85it/s]\u001b[A\n",
      "F=7.22 C=8.09 S=18.00 P=19.00:  34%|██████████████████████████▍                                                  | 11/32 [00:01<00:02,  8.09it/s]\u001b[A\n",
      "F=7.22 C=8.09 S=18.00 P=19.00:  44%|█████████████████████████████████▋                                           | 14/32 [00:01<00:01, 10.83it/s]\u001b[A\n",
      "F=7.22 C=8.09 S=18.00 P=19.00:  50%|██████████████████████████████████████▌                                      | 16/32 [00:02<00:01,  9.75it/s]\u001b[A\n",
      "F=7.22 C=8.09 S=18.00 P=19.00:  59%|█████████████████████████████████████████████▋                               | 19/32 [00:02<00:01, 12.18it/s]\u001b[A\n",
      "F=7.22 C=8.09 S=18.00 P=19.00:  66%|██████████████████████████████████████████████████▌                          | 21/32 [00:02<00:00, 12.80it/s]\u001b[A\n",
      "F=7.22 C=8.09 S=18.00 P=19.00:  72%|███████████████████████████████████████████████████████▎                     | 23/32 [00:02<00:00, 13.71it/s]\u001b[A\n",
      "F=7.22 C=8.09 S=18.00 P=19.00:  78%|████████████████████████████████████████████████████████████▏                | 25/32 [00:02<00:00, 10.17it/s]\u001b[A\n",
      "F=7.22 C=8.09 S=18.00 P=19.00:  84%|████████████████████████████████████████████████████████████████▉            | 27/32 [00:02<00:00, 11.60it/s]\u001b[A\n",
      "F=7.11 C=8.15 S=17.00 P=18.01:  94%|████████████████████████████████████████████████████████████████████████▏    | 30/32 [00:03<00:00, 12.30it/s]\u001b[A\n",
      "F=7.11 C=8.15 S=17.00 P=18.01: 100%|█████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.85it/s]\u001b[A\n",
      "Loss=-0.5490074602509422 - LR=0.01:  76%|█████████████████████████████████████████████████████▍                | 382/500 [00:25<00:07, 14.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping loss difference is smaller than 1e-10\n",
      "traning loss: -0.5490074602509422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "psi_pqc = qmps_f(num_qubits, in_depth= depth, n_Qbit=num_qubits-1, qmps_structure=\"brickwall\", canon=\"left\",val_iden = 0.1,rand = True)\n",
    "pqc = psi_pqc.tensors[num_qubits]\n",
    "for i in range (num_qubits+1,len(psi_pqc.tensors)):\n",
    "    pqc = pqc&psi_pqc.tensors[i] #extrating the pqc part\n",
    "pqc.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.complex128))\n",
    "model = TNModel(pqc,rho,gs)\n",
    "lr = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=200, gamma=0.5)\n",
    "num_steps = 500\n",
    "pbar = tqdm.tqdm(range(num_steps))\n",
    "previous_loss = torch.inf\n",
    "losses = []\n",
    "for step in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    loss = model.forward()\n",
    "    losses.append(loss.detach().numpy())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_description(f\"Loss={loss} - LR={lr}\")\n",
    "    if step > 100 and torch.abs(previous_loss - loss) < 1e-10:\n",
    "        print(\"Early stopping loss difference is smaller than 1e-10\")\n",
    "        break\n",
    "    previous_loss = loss.clone()\n",
    "print(f'traning loss: {loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
